{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "#LOADING EVERYTHING\n",
    "directory = \"/Users/joshs/Desktop/Spring 2021/HUM346/wsj-nyt-oped/NYTimes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing stuff from NYTIMES\n",
    "from pynytimes import NYTAPI\n",
    "import urllib3, json, datetime\n",
    "nyt = NYTAPI(\"nKBpzKBvOYipTxZf1T1NwCsVeXad5xoE\", parse_dates=True)\n",
    "articles = nyt.article_search(\n",
    "    results = 500,\n",
    "    dates = {\n",
    "        \"begin\": datetime.datetime(2020, 12, 1),\n",
    "        \"end\": datetime.datetime(2020, 12, 30)\n",
    "    },\n",
    "    options = {\n",
    "        \"sources\": [\n",
    "            \"New York Times\",\n",
    "        ],\n",
    "        \"news_desk\": [\n",
    "            \"OpEd\",\n",
    "            \"Editorial\"\n",
    "            \n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing articles to be saved\n",
    "for article in articles:\n",
    "    article['abstract'] = re.sub('\\?', '', article['abstract'])\n",
    "    article['abstract'] = re.sub('\\.', '', article['abstract'])\n",
    "    article['abstract'] = re.sub('\\/', '', article['abstract'])\n",
    "    text_file = open(f\"./Articles/{article['abstract']}.txt\", \"w+\")\n",
    "    n = text_file.write(article['lead_paragraph'] + \" \\n\", )\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting text into all lowercase\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text) \n",
    "    return split_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all data together IT SHOULD WORK NOW BUT WAS NOT WORKING EARLIER\n",
    "files = glob.glob(f\"{directory}/Articles/*.txt\")\n",
    "training_data = []\n",
    "for file in files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    processed_text = little_mallet_wrapper.process_string(text)\n",
    "    training_data.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "for file in files:\n",
    "    text = open(file).read()\n",
    "    original_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bc11a219b860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mall_the_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_into_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmeaningful_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_the_words\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmeaningful_words_tally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeaningful_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f61109e12f30>\u001b[0m in \u001b[0;36msplit_into_words\u001b[0;34m(any_chunk_of_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#splitting text into all lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_into_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0many_chunk_of_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlowercase_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0many_chunk_of_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msplit_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\W+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "full_text = training_data\n",
    "number_of_desired_words = 10\n",
    "# Manipulate and Analyze File\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
    "\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('many', 11),\n",
       " ('year', 9),\n",
       " ('new', 9),\n",
       " ('one', 9),\n",
       " ('world', 8),\n",
       " ('pandemic', 8),\n",
       " ('months', 8),\n",
       " ('coronavirus', 7),\n",
       " ('us', 7),\n",
       " ('states', 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_mallet = '/Users/joshs/mallet-2.0.8/bin/mallet'\n",
    "num_topics = 5\n",
    "    \n",
    "#Change to your desired output directory\n",
    "output_directory_path = '/Users/joshs/Desktop/Spring 2021/HUM346/wsj-nyt-oped/NYTimes/topic-model-output'\n",
    "\n",
    "#No need to change anything below here\n",
    "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
    "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
    "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "path_to_topic_distributions     = f\"{output_directory_path}/{str(num_topics)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 3595\n",
      "Mean Number of Words per Document: 26.6\n",
      "Vocabulary Size: 15500\n"
     ]
    }
   ],
   "source": [
    "little_mallet_wrapper.print_dataset_stats(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#Importing Data\n",
    "little_mallet_wrapper.import_data(path_to_mallet,\n",
    "                path_to_training_data,\n",
    "                path_to_formatted_training_data,\n",
    "                training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training topic model...\n"
     ]
    }
   ],
   "source": [
    "#Training Data Doesnt seem to work?\n",
    "little_mallet_wrapper.train_topic_model(path_to_mallet,\n",
    "                      path_to_formatted_training_data,\n",
    "                      path_to_model,\n",
    "                      path_to_topic_keys,\n",
    "                      path_to_topic_distributions,\n",
    "                      num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨Topic 0✨\n",
      "\n",
      "['trump', 'president', 'election', 'donald', 'biden', 'NUM', 'joe', 'political', 'presidential', 'court', 'democratic', 'party', 'state', 'would', 'national', 'last', 'republican', 'justice', 'one', 'supreme']\n",
      "\n",
      "✨Topic 1✨\n",
      "\n",
      "['NUM', 'new', 'years', 'two', 'last', 'year', 'city', 'york', 'ago', 'country', 'week', 'since', 'people', 'police', 'three', 'month', 'first', 'one', 'million', 'percent']\n",
      "\n",
      "✨Topic 2✨\n",
      "\n",
      "['part', 'article', 'sign', 'newsletter', 'receive', 'black', 'american', 'times', 'war', 'women', 'thursdays', 'tuesdays', 'debatable', 'big', 'every', 'world', 'news', 'today', 'one', 'america']\n",
      "\n",
      "✨Topic 3✨\n",
      "\n",
      "['time', 'like', 'one', 'day', 'would', 'home', 'know', 'back', 'never', 'family', 'said', 'people', 'get', 'school', 'life', 'first', 'days', 'even', 'way', 'work']\n",
      "\n",
      "✨Topic 4✨\n",
      "\n",
      "['coronavirus', 'pandemic', 'states', 'NUM', 'covid', 'united', 'people', 'many', 'health', 'americans', 'country', 'crisis', 'america', 'public', 'world', 'even', 'economic', 'care', 'could', 'much']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
    "\n",
    "for topic_number, topic in enumerate(topics):\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
